{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e31465f",
   "metadata": {},
   "source": [
    "## Reducing the dimensionality of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bd00a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f19e61",
   "metadata": {},
   "source": [
    "Whichever method of dimensionality reduction that we choose, we still have to make sure that the lower dimension variables explain a **significant proportion** of the original data.(90%)?.\n",
    "\n",
    "So what happens when we run this algorithm?\n",
    "\n",
    "What happens when we run regression on PCA?\n",
    "\n",
    "We are predicting the dependent variable based on patterns of co-variation in the inputs, rather than individual features. So coefficients are not directly interpretable per variable. Instead, we interpret which group of features each component represents.\n",
    "\n",
    "### With GWR\n",
    "\n",
    "We're fitting a local regression model at each h3 cell here, using sptially weighted data from nearby cells. Instead of one global coefficient for each predictor, you get a surface of coefficients, one per h3, indicating that relationships vary across space.\n",
    "\n",
    "So essentially - GWR tells you where each of your variables are more strongly linked to the dependent variable (in this case, the KPI). So in that sense, perhaps it makes some sense to have some explainability within the PCA.\n",
    "\n",
    "### So... the framework\n",
    "\n",
    "1. Join all H3 datasets → one large GeoDataFrame.\n",
    "2. Standardize / normalize features.\n",
    "3. Apply PCA to reduce dimensionality:\n",
    "    * per thematic block (POIs, buildings, greenness, etc.)\n",
    "4. Fit regression models:\n",
    "    * Global OLS or spatial lag/error regression → for baseline relationships.\n",
    "    * GWR → to see where relationships vary spatially.\n",
    "5. Map local coefficients → interpret which urban features (or principal components) are locally more important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbad9d73",
   "metadata": {},
   "source": [
    "## It's probably best to PCA the datasets indiviudally so the PCA clusters can be conceptually similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb33aacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Suppose you have one thematic table\n",
    "X = buildings_df.drop(columns=['h3_id'])\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca_components = pca.fit_transform(X_scaled)\n",
    "\n",
    "buildings_pca = (\n",
    "    pd.DataFrame(pca_components, columns=[f'build_pc{i+1}' for i in range(3)])\n",
    "    .assign(h3_id=buildings_df['h3_id'].values)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
