{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e31465f",
   "metadata": {},
   "source": [
    "## Reducing the dimensionality of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90bd00a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f19e61",
   "metadata": {},
   "source": [
    "Whichever method of dimensionality reduction that we choose, we still have to make sure that the lower dimension variables explain a **significant proportion** of the original data.(90%)?.\n",
    "\n",
    "So what happens when we run this algorithm?\n",
    "\n",
    "What happens when we run regression on PCA?\n",
    "\n",
    "We are predicting the dependent variable based on patterns of co-variation in the inputs, rather than individual features. So coefficients are not directly interpretable per variable. Instead, we interpret which group of features each component represents.\n",
    "\n",
    "### With GWR\n",
    "\n",
    "We're fitting a local regression model at each h3 cell here, using sptially weighted data from nearby cells. Instead of one global coefficient for each predictor, you get a surface of coefficients, one per h3, indicating that relationships vary across space.\n",
    "\n",
    "So essentially - GWR tells you where each of your variables are more strongly linked to the dependent variable (in this case, the KPI). So in that sense, perhaps it makes some sense to have some explainability within the PCA.\n",
    "\n",
    "> Lasso regression or scikit feature selection for clustering the variables\n",
    "\n",
    "### So... the framework\n",
    "\n",
    "1. Join all H3 datasets → one large GeoDataFrame.\n",
    "2. Standardize / normalize features.\n",
    "3. Apply PCA to reduce dimensionality:\n",
    "    * per thematic block (POIs, buildings, greenness, etc.)\n",
    "4. Fit regression models:\n",
    "    * Global OLS or spatial lag/error regression → for baseline relationships.\n",
    "    * GWR → to see where relationships vary spatially.\n",
    "5. Map local coefficients → interpret which urban features (or principal components) are locally more important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbad9d73",
   "metadata": {},
   "source": [
    "## It's probably best to PCA the datasets indiviudally so the PCA clusters can be conceptually similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5e95d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "repo_root = Path.cwd() if (Path.cwd() / \"src\").exists() else Path.cwd().parent\n",
    "sys.path.append(str(repo_root / \"src\"))\n",
    "\n",
    "from config import PROCESSED_DATA_DIR\n",
    "import geopandas as gpd\n",
    "\n",
    "gdf = gpd.read_parquet(PROCESSED_DATA_DIR / \"barcelona_h3_res10_landuse_aggregated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63d74730",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in datasets individually\n",
    "landuse = gpd.read_parquet('../data/processed/barcelona_h3_res10_landuse_aggregated.parquet')\n",
    "ndvi = gpd.read_parquet('../data/processed/barcelona_h3_res10_ndvi_aggregated.parquet')\n",
    "pois = gpd.read_parquet('../data/processed/barcelona_h3_res10_poi.parquet')\n",
    "buildings = gpd.read_parquet('../data/processed/barcelona_h3_res10_building_aggregated.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39613ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87,)\n",
      "(8,)\n",
      "(359,)\n",
      "(28,)\n"
     ]
    }
   ],
   "source": [
    "print(landuse.columns.shape) # we only want to use the pct area columns here\n",
    "print(ndvi.columns.shape) # I'm assuming here that these dummy columns refer to % of clustered category in each h3 cell.\n",
    "print(pois.columns.shape)\n",
    "print(buildings.columns.shape) # we only want to use the pct area columns here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6643b844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new landuse shape = 17\n",
      "new buildings shape = 13\n"
     ]
    }
   ],
   "source": [
    "#we only want to keep the columns that start with pct_area_ for landuse and buildings\n",
    "landuse_features = [col for col in landuse.columns if col.startswith('pct_area_')]\n",
    "landuse_df = landuse[landuse_features]\n",
    "print(\"new landuse cols = \" + str(landuse_df.shape[1]))\n",
    "buildings_features = [col for col in buildings.columns if col.startswith('pct_area_')]\n",
    "buildings_df = buildings[buildings_features]\n",
    "buildings_df.shape\n",
    "print(\"new buildings cols = \" + str(buildings_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f55624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f718816",
   "metadata": {},
   "source": [
    "> Over 400 columns after joining, with a significant weighting to pois. NDVI is *probably* fine, but the others will need some sort of clustering\n",
    "\n",
    "#### Why don't we cluster them together?\n",
    "\n",
    "If we mix the datasets, we're losing structure and interpretability. We'll be clustering counts with POIs and building types in heterogenous clusters, creating garbage clusters that mean very little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c550d97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module loaded. Use reduce_semantic_features(df, prefix, n_clusters).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Clean column names before embedding\n",
    "# --------------------------------------------------------------\n",
    "def clean_column_name(col):\n",
    "    # Remove prefixes and special patterns common in your dataset\n",
    "    replacements = [\n",
    "        \"num_polygons_\", \"num_buildings_\", \"pct_area_\", \"area_\", \n",
    "        \"avg_polygon_area_\", \"avg_\", \"_mean\", \"_count\", \"_total\"\n",
    "    ]\n",
    "\n",
    "    clean = col\n",
    "    for r in replacements:\n",
    "        clean = clean.replace(r, \"\")\n",
    "    clean = clean.replace(\"_\", \" \")\n",
    "\n",
    "    return clean.strip()\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Auto-generate readable cluster names\n",
    "# --------------------------------------------------------------\n",
    "def generate_cluster_name(columns):\n",
    "    # Extract keywords from cleaned names\n",
    "    cleaned_cols = [clean_column_name(c) for c in columns]\n",
    "\n",
    "    vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "    X = vectorizer.fit_transform(cleaned_cols)\n",
    "\n",
    "    freqs = np.asarray(X.sum(axis=0)).ravel()\n",
    "    vocab = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "    if len(vocab) == 0:\n",
    "        return \"misc\"\n",
    "\n",
    "    top_keywords = vocab[np.argsort(freqs)[-3:]]  # top 3 words\n",
    "    return \"_\".join(top_keywords)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Main function: semantic clustering + aggregation\n",
    "# --------------------------------------------------------------\n",
    "def reduce_semantic_features(df, prefix=\"\", n_clusters=4, model_name=\"all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    df          : pandas DataFrame with your landuse / POIs / buildings\n",
    "    prefix      : filter columns by prefix (\"\" means all numeric)\n",
    "    n_clusters  : number of semantic clusters to produce\n",
    "    model_name  : sentence-transformer model\n",
    "    \"\"\"\n",
    "    print(f\"Loading embedding model: {model_name} ...\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 1. Select only numeric columns\n",
    "    # ---------------------------\n",
    "    numeric_cols = [\n",
    "        c for c in df.columns\n",
    "        if c.startswith(prefix)\n",
    "        and pd.api.types.is_numeric_dtype(df[c])\n",
    "    ]\n",
    "\n",
    "    print(f\"Found {len(numeric_cols)} numeric columns starting with '{prefix}'.\")\n",
    "\n",
    "    if len(numeric_cols) == 0:\n",
    "        raise ValueError(\"No numeric columns matched your prefix.\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 2. Clean names for embedding\n",
    "    # ---------------------------\n",
    "    cleaned_names = [clean_column_name(c) for c in numeric_cols]\n",
    "\n",
    "    # ---------------------------\n",
    "    # 3. Embed column names\n",
    "    # ---------------------------\n",
    "    print(\"Embedding column names...\")\n",
    "    embeddings = model.encode(cleaned_names, show_progress_bar=True)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 4. Cluster embeddings\n",
    "    # ---------------------------\n",
    "    print(f\"Clustering into {n_clusters} semantic groups...\")\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "    # Build cluster -> columns mapping\n",
    "    clusters = {i: [] for i in range(n_clusters)}\n",
    "    for col, label in zip(numeric_cols, labels):\n",
    "        clusters[label].append(col)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 5. Auto name clusters\n",
    "    # ---------------------------\n",
    "    cluster_names = {}\n",
    "    for cluster_id, cols in clusters.items():\n",
    "        cluster_names[cluster_id] = generate_cluster_name(cols)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 6. Aggregate features per cluster\n",
    "    # ---------------------------\n",
    "    out_df = pd.DataFrame(index=df.index)\n",
    "\n",
    "    for cluster_id, col_list in clusters.items():\n",
    "        readable = cluster_names[cluster_id]\n",
    "        new_col = f\"{prefix}cluster_{cluster_id}_{readable}\"\n",
    "        out_df[new_col] = df[col_list].sum(axis=1)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 7. Display summary\n",
    "    # ---------------------------\n",
    "    print(\"\\nGenerated the following aggregated features:\")\n",
    "    for cid, name in cluster_names.items():\n",
    "        print(f\"  - Cluster {cid}: {name}\")\n",
    "        print(f\"    Example columns: {clusters[cid][:4]} ... ({len(clusters[cid])} total)\\n\")\n",
    "\n",
    "    return out_df, clusters, cluster_names\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Optional quick test (comment out when importing)\n",
    "# --------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Module loaded. Use reduce_semantic_features(df, prefix, n_clusters).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6040bd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2 ...\n",
      "Found 17 numeric columns starting with ''.\n",
      "Embedding column names...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering into 4 semantic groups...\n",
      "\n",
      "Generated the following aggregated features:\n",
      "  - Cluster 0: golf\n",
      "    Example columns: ['pct_area_golf'] ... (1 total)\n",
      "\n",
      "  - Cluster 1: military_religious_transportation\n",
      "    Example columns: ['pct_area_agriculture', 'pct_area_education', 'pct_area_entertainment', 'pct_area_managed'] ... (8 total)\n",
      "\n",
      "  - Cluster 2: developed_horticulture_residential\n",
      "    Example columns: ['pct_area_construction', 'pct_area_developed', 'pct_area_horticulture', 'pct_area_residential'] ... (4 total)\n",
      "\n",
      "  - Cluster 3: park_pedestrian_recreation\n",
      "    Example columns: ['pct_area_cemetery', 'pct_area_park', 'pct_area_pedestrian', 'pct_area_recreation'] ... (4 total)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "landuse_reduced, landuse_clusters, landuse_names = reduce_semantic_features(landuse_df, prefix=\"\", n_clusters=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f2628e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2 ...\n",
      "Found 13 numeric columns starting with ''.\n",
      "Embedding column names...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering into 4 semantic groups...\n",
      "\n",
      "Generated the following aggregated features:\n",
      "  - Cluster 0: entertainment_medical_religious\n",
      "    Example columns: ['pct_area_agricultural', 'pct_area_commercial', 'pct_area_education', 'pct_area_entertainment'] ... (6 total)\n",
      "\n",
      "  - Cluster 1: military_service_transportation\n",
      "    Example columns: ['pct_area_industrial', 'pct_area_military', 'pct_area_service', 'pct_area_transportation'] ... (4 total)\n",
      "\n",
      "  - Cluster 2: civic_residential\n",
      "    Example columns: ['pct_area_civic', 'pct_area_residential'] ... (2 total)\n",
      "\n",
      "  - Cluster 3: outbuilding\n",
      "    Example columns: ['pct_area_outbuilding'] ... (1 total)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "buildings_reduced, buildings_clusters, buildings_names = reduce_semantic_features(buildings_df, prefix=\"\", n_clusters=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "36a1f61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2 ...\n",
      "Found 357 numeric columns starting with ''.\n",
      "Embedding column names...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 12/12 [00:01<00:00, 10.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering into 4 semantic groups...\n",
      "\n",
      "Generated the following aggregated features:\n",
      "  - Cluster 0: salon_shop_store\n",
      "    Example columns: ['antique_shop', 'art_craft_hobby_store', 'auto_body_shop', 'b2b_supplier_distributor'] ... (58 total)\n",
      "\n",
      "  - Cluster 1: utility_rental_service\n",
      "    Example columns: ['accountant_or_bookkeeper', 'adoption_service', 'agricultural_service', 'air_transport_facility_service'] ... (108 total)\n",
      "\n",
      "  - Cluster 2: care_animal_school\n",
      "    Example columns: ['allergy_and_immunology', 'alternative_medicine', 'animal_boarding', 'animal_hospital'] ... (67 total)\n",
      "\n",
      "  - Cluster 3: place_station_sport\n",
      "    Example columns: ['accommodation', 'agricultural_area', 'airport', 'airport_terminal'] ... (124 total)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pois_reduced, pois_clusters, pois_names = reduce_semantic_features(pois, prefix=\"\", n_clusters=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb33aacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Suppose you have one thematic table\n",
    "X = buildings_df.drop(columns=['h3_id'])\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca_components = pca.fit_transform(X_scaled)\n",
    "\n",
    "buildings_pca = (\n",
    "    pd.DataFrame(pca_components, columns=[f'build_pc{i+1}' for i in range(3)])\n",
    "    .assign(h3_id=buildings_df['h3_id'].values)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reallocate_AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
